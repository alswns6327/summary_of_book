# 바이트와 16진수

컴퓨터 시스템에서 CPU가 한 번에 처리할 수 있는 단위를 워드라고 한다.
초기에는 이 워드의 단위를 6비트로 하였으나, 이후 컴퓨터에서 2진수가 중요해짐에 따라 2의 거듭제곱으로 표현할 수 있는 8비트 즉 바이트(byte)가 탄생하게 된다.

# ASCII에서 유니코드까지

ASCII란 정보 교환을 위한 미국 표준 부호로 컴퓨터가 이해 가능한 수로 표현한 미국 표준 언어 부호이다.  
128비트를 초과하지 않아 7비트로 설계 되었으나 컴퓨터의 아키텍처를 고려해 8비트 값으로 저장하게 되었다.  
ASCII는 탄생과 동시에 언어 표준이 되었으나 IBM에서는 별도의 부호(EBCDIC)를 사용하고 있었으며, 두 부호 모두 미국을 중심으로 설계되어 다른 나라의 언어나 기호를 나타내기에는 턱 없이 부족하다는 문제를 가지고 있었다.  
이후 ANSI 문자셋이라는 ASCII의 확장판이 나오고 해당 문자셋은 MS의 윈도우에 포함되어 인기를 끌게 된다.  
하지만 너무 많은 확장판이 나오게 됨에 따라 서로 호환되지 않은 여러 문자셋이 생겨나게 되었고, 이 문제를 해결하기 위해 유니코드(Unicode)라는 문자셋을 개발하게 된다.  
유니코드는 초기에 16비트로 설계되었으며, 현재는 21비트를 사용하는 부호로 확장되었다.  
하지만 간단한 문자 가령 'A'라는 문자를 표현하기 위해서는 기존의 ASCII의 범위인 1바이트로도 충분하나 유니코드의 21비트를 위해선 최소 3바이트를 사용해야했다.  
이런 문제를 해결하기 위해서 UTF(유니코드 변환 형식 - Unicode transformation format)이 탄생하게 된다.

- utf-32 : 유니코드를 32비트로 표현하여 추가적인 바이트의 작업이 없으나 낭비되는 부분이 클 수 있다.
- utf-16 : 유니코드를 16비트로 표현하며 16비트를 초과하는 문자에 대해선 16비트를 추가한 4바이트로 표현한다.
- utf-8 : 유니코드를 8비트로 표현하며 동일하게 8비트를 초과하는 문자에 대해선 초과된 만큼 1바이트씩 최대 4바이트로 표현한다.

utf-32는 별다른 인코딩 없이 모든 문자를 담을 수 있고 utf-8은 ANSI와 호환이 되며, utf-16은 한글과 같이 2바이트의 데이터를 표현할 때 다른 포맷보다 더 작은 비트로 표현이 가능하므로 적절하게 선택하면 될 것 같다.
